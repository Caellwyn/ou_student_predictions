{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_timeseries_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaHEvfvp2+JAcGh+hSpH7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caellwyn/ou_student_predictions/blob/deep_models/notebooks/deep_timeseries_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqzFRYjEdOxQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from seaborn import heatmap\n",
        "\n",
        "from keras import Input\n",
        "from keras import backend as K\n",
        "from keras import regularizers, optimizers\n",
        "from keras.layers import Dense, Dropout, Conv1D, \\\n",
        "AveragePooling1D, MaxPooling1D, LSTM, GRU, Flatten\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, \\\n",
        "roc_auc_score, plot_precision_recall_curve, plot_roc_curve, \\\n",
        "plot_confusion_matrix, confusion_matrix\n",
        "\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join(os.pardir))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "try:\n",
        "    from src.functions import add_model, test_model, get_timeseries_table\n",
        "except:\n",
        "    from functions import add_model, test_model, get_timeseries_table"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy5TWGhKoa3r"
      },
      "source": [
        "The below is thanks to [dokondr](https://stackoverflow.com/questions/45411902/how-to-use-f1-score-with-keras-model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyTr0M5foZc8"
      },
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "\n",
        "    # Count positive samples.\n",
        "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "    # If there are no true samples, fix the F1 score at 0.\n",
        "    if c3 == 0:\n",
        "        return 0\n",
        "\n",
        "    # How many selected items are relevant?\n",
        "    precision = c1 / c2\n",
        "\n",
        "    # How many relevant items are selected?\n",
        "    recall = c1 / c3\n",
        "\n",
        "    # Calculate f1_score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1_score "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8yTbGIPdXjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "e7472715-390b-4fbd-bc55-609e1826546e"
      },
      "source": [
        "df = get_timeseries_table(prediction_window=135, \n",
        "                          binary_labels=True, \n",
        "                          one_hot_modules=True)\n",
        "df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>assess_submitted_1</th>\n",
              "      <th>assess_score_1</th>\n",
              "      <th>assess_submitted_2</th>\n",
              "      <th>assess_score_2</th>\n",
              "      <th>assess_submitted_3</th>\n",
              "      <th>assess_score_3</th>\n",
              "      <th>assess_submitted_4</th>\n",
              "      <th>assess_score_4</th>\n",
              "      <th>assess_submitted_5</th>\n",
              "      <th>assess_score_5</th>\n",
              "      <th>assess_submitted_6</th>\n",
              "      <th>assess_score_6</th>\n",
              "      <th>assess_submitted_7</th>\n",
              "      <th>assess_score_7</th>\n",
              "      <th>assess_submitted_8</th>\n",
              "      <th>assess_score_8</th>\n",
              "      <th>sum_activities_-25</th>\n",
              "      <th>sum_click_-25</th>\n",
              "      <th>activities_x_clicks_-25</th>\n",
              "      <th>sum_activities_-24</th>\n",
              "      <th>sum_click_-24</th>\n",
              "      <th>activities_x_clicks_-24</th>\n",
              "      <th>sum_activities_-23</th>\n",
              "      <th>sum_click_-23</th>\n",
              "      <th>activities_x_clicks_-23</th>\n",
              "      <th>sum_activities_-22</th>\n",
              "      <th>sum_click_-22</th>\n",
              "      <th>activities_x_clicks_-22</th>\n",
              "      <th>sum_activities_-21</th>\n",
              "      <th>sum_click_-21</th>\n",
              "      <th>activities_x_clicks_-21</th>\n",
              "      <th>sum_activities_-20</th>\n",
              "      <th>sum_click_-20</th>\n",
              "      <th>activities_x_clicks_-20</th>\n",
              "      <th>sum_activities_-19</th>\n",
              "      <th>sum_click_-19</th>\n",
              "      <th>activities_x_clicks_-19</th>\n",
              "      <th>sum_activities_-18</th>\n",
              "      <th>sum_click_-18</th>\n",
              "      <th>activities_x_clicks_-18</th>\n",
              "      <th>...</th>\n",
              "      <th>sum_click_124</th>\n",
              "      <th>activities_x_clicks_124</th>\n",
              "      <th>sum_activities_125</th>\n",
              "      <th>sum_click_125</th>\n",
              "      <th>activities_x_clicks_125</th>\n",
              "      <th>sum_activities_126</th>\n",
              "      <th>sum_click_126</th>\n",
              "      <th>activities_x_clicks_126</th>\n",
              "      <th>sum_activities_127</th>\n",
              "      <th>sum_click_127</th>\n",
              "      <th>activities_x_clicks_127</th>\n",
              "      <th>sum_activities_128</th>\n",
              "      <th>sum_click_128</th>\n",
              "      <th>activities_x_clicks_128</th>\n",
              "      <th>sum_activities_129</th>\n",
              "      <th>sum_click_129</th>\n",
              "      <th>activities_x_clicks_129</th>\n",
              "      <th>sum_activities_130</th>\n",
              "      <th>sum_click_130</th>\n",
              "      <th>activities_x_clicks_130</th>\n",
              "      <th>sum_activities_131</th>\n",
              "      <th>sum_click_131</th>\n",
              "      <th>activities_x_clicks_131</th>\n",
              "      <th>sum_activities_132</th>\n",
              "      <th>sum_click_132</th>\n",
              "      <th>activities_x_clicks_132</th>\n",
              "      <th>sum_activities_133</th>\n",
              "      <th>sum_click_133</th>\n",
              "      <th>activities_x_clicks_133</th>\n",
              "      <th>sum_activities_134</th>\n",
              "      <th>sum_click_134</th>\n",
              "      <th>activities_x_clicks_134</th>\n",
              "      <th>final_result</th>\n",
              "      <th>module_AAA</th>\n",
              "      <th>module_BBB</th>\n",
              "      <th>module_CCC</th>\n",
              "      <th>module_DDD</th>\n",
              "      <th>module_EEE</th>\n",
              "      <th>module_FFF</th>\n",
              "      <th>module_GGG</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>registration</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AAA2013J11391</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AAA2013J28400</th>\n",
              "      <td>3.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>418.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AAA2013J31604</th>\n",
              "      <td>-2.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>510.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AAA2013J32885</th>\n",
              "      <td>7.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AAA2013J38053</th>\n",
              "      <td>0.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GGG2014J691787</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GGG2014J692171</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GGG2014J693046</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GGG2014J695877</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GGG2014J698019</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24005 rows Ã— 504 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                assess_submitted_1  assess_score_1  ...  module_FFF  module_GGG\n",
              "registration                                        ...                        \n",
              "AAA2013J11391                 -1.0            78.0  ...           0           0\n",
              "AAA2013J28400                  3.0            70.0  ...           0           0\n",
              "AAA2013J31604                 -2.0            72.0  ...           0           0\n",
              "AAA2013J32885                  7.0            69.0  ...           0           0\n",
              "AAA2013J38053                  0.0            79.0  ...           0           0\n",
              "...                            ...             ...  ...         ...         ...\n",
              "GGG2014J691787                 0.0             0.0  ...           0           1\n",
              "GGG2014J692171                 0.0             0.0  ...           0           1\n",
              "GGG2014J693046                 0.0             0.0  ...           0           1\n",
              "GGG2014J695877                 0.0             0.0  ...           0           1\n",
              "GGG2014J698019                 0.0             0.0  ...           0           1\n",
              "\n",
              "[24005 rows x 504 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvzyNQR-oXj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2186c81-b079-4b8f-ee8c-ef19961f5dae"
      },
      "source": [
        "X = df.drop(columns='final_result')\n",
        "y = df['final_result']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111, test_size=.1)\n",
        "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state=111, test_size=.1)\n",
        "\n",
        "categoricals = [502, 501, 500, 499, 498, 497, 496]\n",
        "smotenc = SMOTENC(categoricals, random_state=111)\n",
        "os_X_train, os_y_train = smotenc.fit_resample(X_train, y_train)\n",
        "os_X_t, os_y_t = smotenc.fit_resample(X_t, y_t)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfTKcEOEkt-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9c7ecd0-7c0f-44e5-a9b8-db1d46ff7366"
      },
      "source": [
        "l1 = 1e-4\n",
        "l2 = 1e-3\n",
        "bias = 1e-4\n",
        "dropout500=0.4\n",
        "dropout300=0.2\n",
        "dropout200=0.2\n",
        "\n",
        "model1 = keras.Sequential()\n",
        "model1.add(Input((X.shape[1])))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = optimizers.Adamax()\n",
        "\n",
        "model1.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#class_balance = num(pass) / num(fail).  Use this to weight class 0.\n",
        "class_balance = sum(y_t == 1) / sum(y_t == 0)\n",
        "\n",
        "model1.fit(X_t, y_t,\n",
        "           batch_size = 200,\n",
        "           epochs = 50,\n",
        "           validation_data=(X_val, y_val),\n",
        "        #    class_weight = {1: 1.0,\n",
        "        #                    0: class_balance}\n",
        "           )\n",
        "\n",
        "yhat = np.around(model1.predict(X_val)).astype(int)[:,0]\n",
        "\n",
        "confusion = confusion_matrix(y_val, yhat, normalize='true')\n",
        "heatmap(confusion, cmap='Greens', annot=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "98/98 [==============================] - 5s 13ms/step - loss: 14.8615 - accuracy: 0.6124 - val_loss: 13.8762 - val_accuracy: 0.6446\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 13.6919 - accuracy: 0.6426 - val_loss: 13.2951 - val_accuracy: 0.6391\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 13.1447 - accuracy: 0.6427 - val_loss: 12.8268 - val_accuracy: 0.6719\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 12.6844 - accuracy: 0.6409 - val_loss: 12.3972 - val_accuracy: 0.6784\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 12.2538 - accuracy: 0.6524 - val_loss: 11.9993 - val_accuracy: 0.5983\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 11.8485 - accuracy: 0.6685 - val_loss: 11.5950 - val_accuracy: 0.5905\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 11.4481 - accuracy: 0.6747 - val_loss: 11.1914 - val_accuracy: 0.5729\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 11.0407 - accuracy: 0.6808 - val_loss: 10.7939 - val_accuracy: 0.5437\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 10.6313 - accuracy: 0.6898 - val_loss: 10.3957 - val_accuracy: 0.5220\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 10.2223 - accuracy: 0.6926 - val_loss: 9.9828 - val_accuracy: 0.5271\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 9.8025 - accuracy: 0.6965 - val_loss: 9.5457 - val_accuracy: 0.5659\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 9.3773 - accuracy: 0.6999 - val_loss: 9.0991 - val_accuracy: 0.6219\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 8.9386 - accuracy: 0.7124 - val_loss: 8.6419 - val_accuracy: 0.6562\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 8.4742 - accuracy: 0.7362 - val_loss: 8.1370 - val_accuracy: 0.7409\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 8.0271 - accuracy: 0.7429 - val_loss: 7.6812 - val_accuracy: 0.7668\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 7.5609 - accuracy: 0.7637 - val_loss: 7.2336 - val_accuracy: 0.7719\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 7.1055 - accuracy: 0.7694 - val_loss: 6.8263 - val_accuracy: 0.7390\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 6.6539 - accuracy: 0.7719 - val_loss: 6.3166 - val_accuracy: 0.7807\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 6.2008 - accuracy: 0.7768 - val_loss: 5.8995 - val_accuracy: 0.7617\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 5.7563 - accuracy: 0.7807 - val_loss: 5.4579 - val_accuracy: 0.7608\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 5.3308 - accuracy: 0.7781 - val_loss: 5.0347 - val_accuracy: 0.7608\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 4.8987 - accuracy: 0.7857 - val_loss: 4.6242 - val_accuracy: 0.7635\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 4.4831 - accuracy: 0.7862 - val_loss: 4.2365 - val_accuracy: 0.7594\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 4.0929 - accuracy: 0.7864 - val_loss: 3.8189 - val_accuracy: 0.7793\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 3.7201 - accuracy: 0.7810 - val_loss: 3.4741 - val_accuracy: 0.7742\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 3.3633 - accuracy: 0.7855 - val_loss: 3.1072 - val_accuracy: 0.7904\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 3.0170 - accuracy: 0.7966 - val_loss: 2.8173 - val_accuracy: 0.7746\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 2.7156 - accuracy: 0.7895 - val_loss: 2.5266 - val_accuracy: 0.7746\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 2.4254 - accuracy: 0.7900 - val_loss: 2.2489 - val_accuracy: 0.7830\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 2.1755 - accuracy: 0.7899 - val_loss: 2.0004 - val_accuracy: 0.7913\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.9409 - accuracy: 0.7930 - val_loss: 1.7885 - val_accuracy: 0.7959\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.7360 - accuracy: 0.7937 - val_loss: 1.6043 - val_accuracy: 0.7955\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.5529 - accuracy: 0.7930 - val_loss: 1.4459 - val_accuracy: 0.7881\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.3924 - accuracy: 0.7992 - val_loss: 1.2990 - val_accuracy: 0.7950\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.2496 - accuracy: 0.8015 - val_loss: 1.1822 - val_accuracy: 0.7950\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.1463 - accuracy: 0.7973 - val_loss: 1.0932 - val_accuracy: 0.7797\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.0324 - accuracy: 0.8052 - val_loss: 0.9965 - val_accuracy: 0.7871\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.9588 - accuracy: 0.8022 - val_loss: 0.9257 - val_accuracy: 0.7894\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.8876 - accuracy: 0.8007 - val_loss: 0.8618 - val_accuracy: 0.7904\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.8326 - accuracy: 0.8012 - val_loss: 0.8134 - val_accuracy: 0.7918\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.7905 - accuracy: 0.7995 - val_loss: 0.7772 - val_accuracy: 0.7918\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.7520 - accuracy: 0.8025 - val_loss: 0.7421 - val_accuracy: 0.7969\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.7224 - accuracy: 0.7986 - val_loss: 0.7160 - val_accuracy: 0.7927\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6914 - accuracy: 0.8016 - val_loss: 0.6986 - val_accuracy: 0.7871\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6687 - accuracy: 0.8021 - val_loss: 0.6768 - val_accuracy: 0.7913\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6445 - accuracy: 0.8139 - val_loss: 0.6662 - val_accuracy: 0.7899\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6351 - accuracy: 0.8122 - val_loss: 0.6561 - val_accuracy: 0.7927\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6159 - accuracy: 0.8176 - val_loss: 0.6517 - val_accuracy: 0.7894\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6145 - accuracy: 0.8121 - val_loss: 0.6482 - val_accuracy: 0.7853\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6103 - accuracy: 0.8115 - val_loss: 0.6457 - val_accuracy: 0.7881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9322793410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yzCZCt9hW16E",
        "outputId": "15bf7356-97d0-4abd-b386-68ca239b3865"
      },
      "source": [
        "l1 = 1e-4\n",
        "l2 = 1e-3\n",
        "bias = 1e-4\n",
        "dropout500=0.4\n",
        "dropout300=0.2\n",
        "dropout200=0.2\n",
        "\n",
        "model1 = keras.Sequential()\n",
        "model1.add(Input((X.shape[1])))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate = .001)\n",
        "\n",
        "model1.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "class_balance = sum(y_t == 1) / sum(y_t == 0)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0001)\n",
        "\n",
        "model1.fit(X_t, y_t,\n",
        "           batch_size = 200,\n",
        "           epochs = 50,\n",
        "           validation_data=(X_val, y_val),\n",
        "           callbacks = [reduce_lr]\n",
        "           )\n",
        "\n",
        "#class_balance = num(pass) / num(fail).  Use this to weight class 0.\n",
        "class_balance = sum(y_t == 1) / sum(y_t == 0)\n",
        "\n",
        "model1.fit(X_t, y_t,\n",
        "           batch_size = 200,\n",
        "           epochs = 50,\n",
        "           validation_data=(X_val, y_val),\n",
        "        #    class_weight = {1: 1.0,\n",
        "        #                    0: class_balance}\n",
        "           )\n",
        "\n",
        "yhat = np.around(model1.predict(X_val)).astype(int)[:,0]\n",
        "\n",
        "confusion = confusion_matrix(y_val, yhat, normalize='true')\n",
        "heatmap(confusion, cmap='Greens', annot=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "98/98 [==============================] - 3s 11ms/step - loss: 14.6885 - accuracy: 0.6125 - val_loss: 12.2887 - val_accuracy: 0.6391\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 11.6966 - accuracy: 0.6386 - val_loss: 10.1294 - val_accuracy: 0.6858\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 9.6047 - accuracy: 0.6800 - val_loss: 8.2058 - val_accuracy: 0.7210\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 7.7759 - accuracy: 0.7317 - val_loss: 6.5753 - val_accuracy: 0.7483\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 6.2198 - accuracy: 0.7652 - val_loss: 5.2493 - val_accuracy: 0.7733\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 4.9693 - accuracy: 0.7728 - val_loss: 4.1943 - val_accuracy: 0.7765\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 3.9725 - accuracy: 0.7792 - val_loss: 3.3620 - val_accuracy: 0.7848\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 3.1869 - accuracy: 0.7832 - val_loss: 2.7420 - val_accuracy: 0.7760\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 2.5874 - accuracy: 0.7812 - val_loss: 2.2344 - val_accuracy: 0.7811\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 2.1181 - accuracy: 0.7829 - val_loss: 1.8498 - val_accuracy: 0.7756\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 1.7539 - accuracy: 0.7879 - val_loss: 1.5487 - val_accuracy: 0.7834\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 1.4807 - accuracy: 0.7861 - val_loss: 1.3138 - val_accuracy: 0.7881\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 1.2655 - accuracy: 0.7897 - val_loss: 1.1615 - val_accuracy: 0.7705\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 1.0963 - accuracy: 0.7985 - val_loss: 1.0215 - val_accuracy: 0.7825\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.9857 - accuracy: 0.7878 - val_loss: 0.9154 - val_accuracy: 0.7899\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.8890 - accuracy: 0.7916 - val_loss: 0.8578 - val_accuracy: 0.7691\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.8156 - accuracy: 0.7918 - val_loss: 0.7972 - val_accuracy: 0.7746\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.7696 - accuracy: 0.7912 - val_loss: 0.7386 - val_accuracy: 0.7890\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.7139 - accuracy: 0.7932 - val_loss: 0.7063 - val_accuracy: 0.7839\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6893 - accuracy: 0.7898 - val_loss: 0.6814 - val_accuracy: 0.7802\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6600 - accuracy: 0.7903 - val_loss: 0.6468 - val_accuracy: 0.7867\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6357 - accuracy: 0.7951 - val_loss: 0.6294 - val_accuracy: 0.7894\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.6132 - accuracy: 0.7978 - val_loss: 0.6168 - val_accuracy: 0.7871\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5974 - accuracy: 0.7994 - val_loss: 0.6113 - val_accuracy: 0.7793\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5960 - accuracy: 0.7910 - val_loss: 0.5971 - val_accuracy: 0.7913\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5870 - accuracy: 0.7976 - val_loss: 0.5889 - val_accuracy: 0.7950\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5802 - accuracy: 0.7960 - val_loss: 0.5809 - val_accuracy: 0.7936\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5661 - accuracy: 0.8022 - val_loss: 0.5857 - val_accuracy: 0.7844\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5624 - accuracy: 0.8034 - val_loss: 0.5833 - val_accuracy: 0.7760\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5616 - accuracy: 0.8005 - val_loss: 0.5739 - val_accuracy: 0.7894\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5620 - accuracy: 0.7996 - val_loss: 0.5854 - val_accuracy: 0.7788\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5635 - accuracy: 0.7976 - val_loss: 0.5759 - val_accuracy: 0.7862\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5653 - accuracy: 0.7968 - val_loss: 0.5842 - val_accuracy: 0.7834\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5625 - accuracy: 0.7967 - val_loss: 0.5734 - val_accuracy: 0.7927\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5505 - accuracy: 0.8026 - val_loss: 0.5728 - val_accuracy: 0.7908\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5522 - accuracy: 0.8032 - val_loss: 0.5785 - val_accuracy: 0.7918\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5513 - accuracy: 0.8045 - val_loss: 0.5669 - val_accuracy: 0.7950\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5554 - accuracy: 0.7978 - val_loss: 0.5773 - val_accuracy: 0.7830\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5372 - accuracy: 0.8121 - val_loss: 0.5684 - val_accuracy: 0.7890\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5515 - accuracy: 0.8036 - val_loss: 0.5886 - val_accuracy: 0.7774\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5478 - accuracy: 0.8039 - val_loss: 0.5754 - val_accuracy: 0.7899\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5538 - accuracy: 0.8043 - val_loss: 0.5888 - val_accuracy: 0.7802\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5489 - accuracy: 0.8017 - val_loss: 0.5630 - val_accuracy: 0.7844\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5266 - accuracy: 0.8113 - val_loss: 0.5560 - val_accuracy: 0.7932\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5221 - accuracy: 0.8150 - val_loss: 0.5562 - val_accuracy: 0.7862\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5162 - accuracy: 0.8151 - val_loss: 0.5547 - val_accuracy: 0.7908\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5170 - accuracy: 0.8137 - val_loss: 0.5527 - val_accuracy: 0.7913\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5144 - accuracy: 0.8181 - val_loss: 0.5490 - val_accuracy: 0.7927\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5121 - accuracy: 0.8184 - val_loss: 0.5556 - val_accuracy: 0.7876\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5099 - accuracy: 0.8175 - val_loss: 0.5536 - val_accuracy: 0.7890\n",
            "Epoch 1/50\n",
            "98/98 [==============================] - 1s 8ms/step - loss: 0.5067 - accuracy: 0.8205 - val_loss: 0.5523 - val_accuracy: 0.7885\n",
            "Epoch 2/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5018 - accuracy: 0.8239 - val_loss: 0.5532 - val_accuracy: 0.7913\n",
            "Epoch 3/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.5029 - accuracy: 0.8222 - val_loss: 0.5529 - val_accuracy: 0.7899\n",
            "Epoch 4/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5022 - accuracy: 0.8241 - val_loss: 0.5520 - val_accuracy: 0.7890\n",
            "Epoch 5/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.5004 - accuracy: 0.8221 - val_loss: 0.5492 - val_accuracy: 0.7848\n",
            "Epoch 6/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4971 - accuracy: 0.8263 - val_loss: 0.5545 - val_accuracy: 0.7876\n",
            "Epoch 7/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4993 - accuracy: 0.8226 - val_loss: 0.5505 - val_accuracy: 0.7881\n",
            "Epoch 8/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4969 - accuracy: 0.8244 - val_loss: 0.5530 - val_accuracy: 0.7922\n",
            "Epoch 9/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4911 - accuracy: 0.8305 - val_loss: 0.5551 - val_accuracy: 0.7885\n",
            "Epoch 10/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4969 - accuracy: 0.8253 - val_loss: 0.5547 - val_accuracy: 0.7904\n",
            "Epoch 11/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4927 - accuracy: 0.8265 - val_loss: 0.5559 - val_accuracy: 0.7899\n",
            "Epoch 12/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4949 - accuracy: 0.8260 - val_loss: 0.5545 - val_accuracy: 0.7959\n",
            "Epoch 13/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4897 - accuracy: 0.8298 - val_loss: 0.5533 - val_accuracy: 0.7904\n",
            "Epoch 14/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4900 - accuracy: 0.8284 - val_loss: 0.5530 - val_accuracy: 0.7885\n",
            "Epoch 15/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4859 - accuracy: 0.8317 - val_loss: 0.5585 - val_accuracy: 0.7881\n",
            "Epoch 16/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4874 - accuracy: 0.8321 - val_loss: 0.5612 - val_accuracy: 0.7899\n",
            "Epoch 17/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4901 - accuracy: 0.8300 - val_loss: 0.5675 - val_accuracy: 0.7890\n",
            "Epoch 18/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4869 - accuracy: 0.8337 - val_loss: 0.5674 - val_accuracy: 0.7834\n",
            "Epoch 19/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4870 - accuracy: 0.8321 - val_loss: 0.5628 - val_accuracy: 0.7876\n",
            "Epoch 20/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4907 - accuracy: 0.8311 - val_loss: 0.5606 - val_accuracy: 0.7890\n",
            "Epoch 21/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4878 - accuracy: 0.8328 - val_loss: 0.5682 - val_accuracy: 0.7904\n",
            "Epoch 22/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4932 - accuracy: 0.8319 - val_loss: 0.5683 - val_accuracy: 0.7839\n",
            "Epoch 23/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4907 - accuracy: 0.8345 - val_loss: 0.5842 - val_accuracy: 0.7816\n",
            "Epoch 24/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4946 - accuracy: 0.8339 - val_loss: 0.5685 - val_accuracy: 0.7904\n",
            "Epoch 25/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4924 - accuracy: 0.8356 - val_loss: 0.5850 - val_accuracy: 0.7844\n",
            "Epoch 26/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4980 - accuracy: 0.8331 - val_loss: 0.5728 - val_accuracy: 0.7876\n",
            "Epoch 27/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4883 - accuracy: 0.8388 - val_loss: 0.5878 - val_accuracy: 0.7839\n",
            "Epoch 28/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4922 - accuracy: 0.8364 - val_loss: 0.5876 - val_accuracy: 0.7862\n",
            "Epoch 29/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4967 - accuracy: 0.8339 - val_loss: 0.5788 - val_accuracy: 0.7881\n",
            "Epoch 30/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4940 - accuracy: 0.8380 - val_loss: 0.5799 - val_accuracy: 0.7844\n",
            "Epoch 31/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4985 - accuracy: 0.8361 - val_loss: 0.5848 - val_accuracy: 0.7871\n",
            "Epoch 32/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4965 - accuracy: 0.8395 - val_loss: 0.5783 - val_accuracy: 0.7853\n",
            "Epoch 33/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4963 - accuracy: 0.8358 - val_loss: 0.5847 - val_accuracy: 0.7848\n",
            "Epoch 34/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4962 - accuracy: 0.8388 - val_loss: 0.5839 - val_accuracy: 0.7807\n",
            "Epoch 35/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4911 - accuracy: 0.8407 - val_loss: 0.5884 - val_accuracy: 0.7848\n",
            "Epoch 36/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4910 - accuracy: 0.8423 - val_loss: 0.5900 - val_accuracy: 0.7848\n",
            "Epoch 37/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4886 - accuracy: 0.8413 - val_loss: 0.5916 - val_accuracy: 0.7881\n",
            "Epoch 38/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4909 - accuracy: 0.8411 - val_loss: 0.5917 - val_accuracy: 0.7848\n",
            "Epoch 39/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4894 - accuracy: 0.8415 - val_loss: 0.5859 - val_accuracy: 0.7885\n",
            "Epoch 40/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4830 - accuracy: 0.8480 - val_loss: 0.5986 - val_accuracy: 0.7844\n",
            "Epoch 41/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4863 - accuracy: 0.8465 - val_loss: 0.5977 - val_accuracy: 0.7867\n",
            "Epoch 42/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4867 - accuracy: 0.8443 - val_loss: 0.5963 - val_accuracy: 0.7825\n",
            "Epoch 43/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4809 - accuracy: 0.8470 - val_loss: 0.5999 - val_accuracy: 0.7811\n",
            "Epoch 44/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4843 - accuracy: 0.8468 - val_loss: 0.6000 - val_accuracy: 0.7830\n",
            "Epoch 45/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4885 - accuracy: 0.8454 - val_loss: 0.6066 - val_accuracy: 0.7867\n",
            "Epoch 46/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4891 - accuracy: 0.8461 - val_loss: 0.5963 - val_accuracy: 0.7885\n",
            "Epoch 47/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4802 - accuracy: 0.8496 - val_loss: 0.6037 - val_accuracy: 0.7853\n",
            "Epoch 48/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4810 - accuracy: 0.8496 - val_loss: 0.6084 - val_accuracy: 0.7834\n",
            "Epoch 49/50\n",
            "98/98 [==============================] - 1s 7ms/step - loss: 0.4816 - accuracy: 0.8509 - val_loss: 0.5940 - val_accuracy: 0.7839\n",
            "Epoch 50/50\n",
            "98/98 [==============================] - 1s 6ms/step - loss: 0.4823 - accuracy: 0.8487 - val_loss: 0.6185 - val_accuracy: 0.7807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f932df31ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD6CAYAAAAC5pRVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYN0lEQVR4nO3de5zVdb3v8dd7LUQ2twNy9TCobEUN3aiFmNZJTEmwAi9ZaJl23E47pcy2HTHNU5R5K/dpJ2lTke1TitdqbCPoFtETgs5YXgKFRlQYErmb3ByG+Zw/ZjmtGYZZM7LmN4sf7yeP78P1+/2+6/v7/h7O4z3f+f5uigjMzCwZma7ugJnZvsSha2aWIIeumVmCHLpmZgly6JqZJciha2aWIIeumdluSJogaamkGknTWtl+sKTHJL0gab6ksoJtdvZ1uhpf5guBbRfb5izr6i5YCeqR7ak9baMjmROP1u52f5KywDJgPFALVAHnRcSSvDr3Ab+PiF9K+ijwhYi4oK19eqRrZta6sUBNRCyPiDpgFjC5RZ1RwLzc58db2b4Lh66ZpYvU7iKpXFJ1XinPa2kYsDJvuTa3Lt/zwNm5z2cBfSQNaKt73fb0+MzMSkq2/TMUEVEBVOzB3q4EbpN0EfAksArY2dYXHLpmli57PCvcZBUwPG+5LLeuSUT8ldxIV1Jv4JyI2NRWo55eMLN06cD0QgFVwEhJIyR1B6YAlc13pYGS3s3Rq4GZhRp16JpZumQ6UNoQEfXAVGAu8BJwb0QsljRd0qRctXHAUknLgCHA9YW65+kFM0uXwiPYdouI2cDsFuuuy/t8P3B/R9p06JpZuhQvczuFQ9fM0qUDVy90BYeumaVLEacXOoND18zSpbQz16FrZimTKe3UdeiaWbqUduY6dM0sZbKlffuBQ9fM0sUjXTOzBPnqBTOzBJV25jp0zSxlfPWCmVmCSjtzHbpmljK+DdjMLEE+kWZmlqDSzlyHrpmljEe6ZmYJKu0b0hy6ZpYyJX7JWIn/TjAz66CM2l8KkDRB0lJJNZKmtbL9IEmPS/qTpBcknVGwe+/xsMzMSlOR3gYsKQvMACYCo4DzJI1qUe1aGl9YeRyNbwv+caHuOXTNLF3UgdK2sUBNRCyPiDpgFjC5RZ0A+uY+/zfgr4Ua9ZyumaWKOnD1gqRyoDxvVUVEVOQ+DwNW5m2rBU5o0cS3gEckfRnoBZxWaJ8OXTNLlY6EbkNjwFYUrLh75wF3RsQPJJ0I/F9JR0dEw+6+4NA1s1TJFu/qhVXA8Lzlsty6fBcDEwAiYqGkHsBAYM3uGvWcrpmliqR2lwKqgJGSRkjqTuOJssoWdVYAp+b2+z6gB7C2rUY90jWzVOnI9EJbIqJe0lRgLpAFZkbEYknTgeqIqAT+FfippCtoPKl2UUREW+06dM0sVYoVugARMRuY3WLddXmflwAf6kibDl0zS5USf/SCQ9fM0qWYI93O4NA1s1TJqLSvD3DomlmqeKRrZpagEs9ch66ZpUumxFPXoWtmqeLpBTOzBGVK/CHmDl0zSxWPdM3MEuTQNTNLkEPXzCxBDl0zswSVeOY6dM0sXTIZ3wZsZpYY3xxhZpagEs9cv66nWE4fM46XZz7BX+78A1d95rJdth80eBj/dfMsnv/Jozz+/fsYNvDApm0Pf+9XbPzNYh76zp0J9tg6y4L/t4BJZ5zJJ06fxM9/OnOX7XV1dXz9a1fxidMn8dnPXMCqVY1v7d5Rt4NvfuN/c87kczn3rE9T9Ux103e+VH4Z5571ac765Dl851vfZefOnYkdz96miK/r6RQO3SLIZDLM+PJ3mfiNCxj1z6dw3imTed9BI5vV+f4Xv8l/PHo/x3xxPNN/9W/ccPG0pm233Hc7F9x0edLdtk6wc+dOvvfdG/nxT27jNw89wJzZc3il5pVmdX7zwG/p27cPv59byecu/Cz/5wc/BOCB+x9s/O/v7uOOn93BD26+lYaGxpfK3nLrTdz3m3t5sPJ+Nm7cyCNzH032wPYi6sC/rlAwdCUdKekqSf+eK1flXsBmOWOPOJaav77Gq6tXsKN+B7Pm/47JJ32sWZ1RB41k3nMLAHj8uaeYfOLft8/70wLe3rol0T5b5/jzi39m+EHDKRtexn7d92PCxNOZP29+szqPz5vPpDM/CcD4j53GM4ueISJY/spyxn7weAAGDDiAPn36sPjPSwDo3bs3APX19ezYUV/yl0V1pWKOdCVNkLRUUo2kaa1s/zdJz+XKMkmbCrXZZuhKugqYBQh4JlcE3N1aB/ZVwwYeyMq1bzQt165b3Wz6AOD55S9x9ofPAOCsD0+kb68+HNCnX6L9tM635s01DB06pGl58NAhvLlmbSt1hgLQrVs3evfpzaZNmzj8iMN5Yt4T1NfXU1u7ipeWLOHN1aubvvcvl1zKKf/jVHr16sn4j52WzAHthTIZtbu0RVIWmAFMBEYB50kalV8nIq6IiGMj4ljgR8CDBftXYPvFwPERcWNE/CpXbgTG5rbtrrPlkqolVVPrERzAlRXf4eTRH+SPt8/h5NEfpHbtG+zM/eloBnDm2ZMZMnQI55/7WW654RaOOfYYMpls0/Y7fvpjHnviUerq6njm6aou7GlpK+JIdyxQExHLI6KOxgHo5DbqnwfcXajRQlcvNAD/HXi9xfoDc9taFREVQAWAxpe1+TriNFi17g2GD/r7yLZs4FBWrXujWZ031r/JOd++BIBePXpyzofP4K0tf0u0n9b5Bg8ZzOrVbzYtr1n9JkMGD2qlzmqGDB1CfX09m9/eTL9+/ZDE16dd2VTv8+dfyMGHHNTsu/vvvz+nfHQcj8+bz4knfbBTj2Vv1ZGpF0nlQHneqopcfgEMA1bmbasFTthNOwcDI4B5hfZZaKT7VeAxSQ9LqsiVOcBjgM/85FQtfZ6Rw0ZwyNDh7NdtP6aMm0zlwuYnOgb07d/0w3D1eVOZOfeeruiqdbKjjj6KFa+voLZ2FTvqdjDn4bmcfMq4ZnXGnXIylb99CIBHH/kvxp5wPJLYtm0bW7duA2DhU4vIZrMcetihbN2ylbVrG6co6uvrefKJPzBixCHJHdRepiMj3YioiIgxeaWi8B5aNQW4PyIKXlbS5kg3IuZIOpzGYfaw3OpVQFV7Gt9X7GzYydTbvsncG35NNpNh5tx7WPL6Mr594ZVUL3uehxY+yrhjTuKGi6cRETz54tNc9qNrmr7/5K0PcOTww+j9D71YeVcVF996JY9UP9GFR2TvVbdu3bj6mqv40iWX0tDQwJlnTeawkYcy40c/5qijRjHuo+M465wzueaqa/nE6ZPo268vN3//RgA2bNjIly65lEwmw+DBg7j+xu8CsG3bNi6/7KvU1e2goaGB48eO4dzPfKoLj7K0FfEc4ypgeN5yWW5da6YAu14r2gpFdO5f//vC9IJ13LY5y7q6C1aCemR77nFkvu+HZ7Q7c166fPZu9yepG7AMOJXcYBM4PyIWt6h3JDAHGBHtCFTfkWZmqVKsy+kiol7SVGAukAVmRsRiSdOB6oiozFWdAsxqT+CCQ9fMUqaYlzBHxGxgdot117VY/lZH2nTomlmqlPqNIw5dM0sVh66ZWYIcumZmCfIr2M3MkuSRrplZcjy9YGaWoBLPXIeumaWLR7pmZgly6JqZJchXL5iZJcgjXTOzBDl0zcwS5NA1M0uQQ9fMLEE+kWZmliCPdM3MElTqoVvobcBmZnsVqf2lcFuaIGmppBpJ03ZT59OSlkhaLOmuQm16pGtmqVKska6kLDADGA/UAlWSKiNiSV6dkcDVwIciYqOkwYXa9UjXzNKleEPdsUBNRCyPiDpgFjC5RZ1LgBkRsREgItYUatSha2apks2o3UVSuaTqvFKe19QwYGXecm1uXb7DgcMlLZC0SNKEQv3z9IKZpUpHphciogKo2IPddQNGAuOAMuBJSf8UEZva+oKZWWpkinf1wipgeN5yWW5dvlrg6YjYAbwqaRmNIVy12/4Vq3dmZqVAUrtLAVXASEkjJHUHpgCVLer8lsZRLpIG0jjdsLytRj3SNbNUKdZIMiLqJU0F5gJZYGZELJY0HaiOiMrcto9JWgLsBL4eEevbateha2apks0U7w/4iJgNzG6x7rq8zwF8LVfaxaFrZqlSxDndTuHQNbNUKfXbgB26ZpYqpX51gEPXzFLF0wtmZgny9IKZWYKyDl0zs+R4esHMLEEOXTOzBHlO18wsQR7pmpklqLQj16FrZinTrYjPXugMDl0zSxXP6ZqZJchzumZmCSrtyHXomlnKeKRrZpagYj7EvDM4dM0sVUo7cku/f2ZmHVLEF1MiaYKkpZJqJE1rZftFktZKei5X/rlQmx7pmlmqFGtOV1IWmAGMp/FV61WSKiNiSYuq90TE1Pa269A1s1Qp4om0sUBNRCwHkDQLmAy0DN0O6fTQ/cH3L+/sXdhe6AuP/GtXd8FK0N0Tb9/jNjpyc4SkcqA8b1VFRFTkPg8DVuZtqwVOaKWZcyR9BFgGXBERK1up08QjXTNLlazaf6oqF7AVBSvu3kPA3RHxjqQvAr8EPtrWF3wizcxSJSO1uxSwChiet1yWW9ckItZHxDu5xZ8BHyjYvw4ci5lZyVMH/hVQBYyUNEJSd2AKUNlsX9KBeYuTgJcKNerpBTNLlWI98CYi6iVNBeYCWWBmRCyWNB2ojohK4CuSJgH1wAbgokLtOnTNLFWKeRtwRMwGZrdYd13e56uBqzvSpkPXzFJFJT5r6tA1s1TxsxfMzBLUjhNkXcqha2ap4kc7mpklyK/rMTNLUMYn0szMkpPxiTQzs+RkfCLNzCw5ntM1M0uQr14wM0uQr9M1M0tQpgPP0+0KDl0zSxWHrplZgjyna2aWIM/pmpklyCNdM7MEyXO6ZmbJKfXphdL+lWBm1kHZTKbdpRBJEyQtlVQjaVob9c6RFJLGFGrTI10zS5ViPXtBUhaYAYwHaoEqSZURsaRFvT7A5cDT7eufmVmKSGp3KWAsUBMRyyOiDpgFTG6l3neAm4Dt7emfQ9fMUkXKdKCoXFJ1XinPa2oYsDJvuTa3Lm9fej8wPCL+s7398/SCmaVKR6YXIqICqHgv+1HjZRK3Ahd15HsOXTNLlSLeBrwKGJ63XJZb964+wNHA/NxUxVCgUtKkiKjeXaMOXTNLlSI+T7cKGClpBI1hOwU4/92NEfEWMDBvv/OBK9sKXHDomlnKFOvqhYiolzQVmAtkgZkRsVjSdKA6IirfS7sOXTNLlWLekRYRs4HZLdZdt5u649rTpkPXzFKl1O9Ic+iaWar4HWlmZgnyQ8zNzBLkV7CbmSXI0wtmZglSiT/dwKFrZqnika6ZWYKyPpFmZpYcX6drZpYgTy+YmSXIJ9LMzBLkka6ZWYJ8c4SZWYJ8G7CZWYI8vWBmliCfSDMzS1DGI919w4rnVvLULxYRDcGRpx7BcWce02q95Yte5dFbH+PsGyYz6NBBTevfXreZe6+4nzHnvp9jJo1OqtvWyY4ZOIrPv+/TZCQer11A5fJHmm3/yLAP8tkjz2bD9k0APPL6EzxeuwCAAT36U/5Pn2NAj/4EcFP1bazbtiHpQ9jrFPPmCEkTgB/S+Lqen0XEjS22/wtwGbAT2AyUR8SSttp06BZBQ0MDC37+FB+/diK9BvTiwat/xyFjDqJ/Wf9m9eq21fHiw4sZPHLQLm0s/OUiDjpu+C7rbe8lxBeOmsL3nvl31m/fyPUnTePZNS+wavPqZvUWvvEsdy65Z5fvXzr6In77ysO8uP5l9s/uT0RDUl3fqxVrTldSFpgBjAdqgSpJlS1C9a6IuCNXfxKNr2Sf0Fa7pT35sZdYU7OWvkP70ndIX7Ldshx20j/yWtXru9SruudZjp08mux+2WbrX33mNfoM7kP/sn5JddkScFi/Q1i9ZS1rtq1jZ+xk4RvVjBnc+l9ALQ3rPZSMMry4/mUA3tn5DnUNOzqzu6mRUabdpYCxQE1ELI+IOmAWMDm/QkT8LW+xFxAF+9fB47FWbN2wld4DejUt9xrQiy0btjars3b5Oras28LB7z+o2fod23fw3O9eYMy570+kr5ac/j36sX77xqbl9ds30r/Hrr9Yxw45jps+dA1fPe4SDujR+NfRgT2HsLV+G1ccV84NH/oG5x9xdsk/U6BUZDrwT1K5pOq8Up7X1DBgZd5ybW5dM5Iuk/QKcDPwlcL9e48kfaGNbU0HsvD+Re91F6kRDcHC/1jEiZ8/YZdt1ff+kdEfP5r9euzXBT2zrvbHNS/ylSeu5aoF1/Piupe4dPSFAGQyGY7sfxi/fvlBrnnqRgb3HMjJZSd2cW/3DpLaXSKiIiLG5JWKju4vImZExKHAVcC1hervyZzut4Ff7KYTFUAFwK3P31JwuL2363lATzav39K0vGX9Fnod0LNpuW77Djau3Ejlt/8TgG2btjHn5keZ8L/Gs6ZmDcuffpVFv36Gui11SCLbPcvRE45K/DisuDZu38SAHn+f1x/Qoz8bcyfM3rV5x99/buatXMD5R5wNwIbtm3j97ZWs2bYOgOo3n2Nkv39kPk8l0PO9WxH/IlgF5J9oKcut251ZwO2FGm0zdCW9sLtNwJBCje8rBh86iLfe+Bt/W/M2vQ7oSc1Tyzn1K6c0bd+/Z3cu/PkFTcuV3/o9J15wAoMOHcTk6Z9sWl9977Ps12M/B25KvPLW6wztNZhB/zCADds3ceKBY7jt+ZnN6vTbvy+b3mmcFvzAkNGs2tJ4ku2VTa/Rs1tP+nTvzdt1mzlqwBEsf2tF4sewNyrizRFVwEhJI2gM2ynA+S32NTIi/pJb/DjwFwooNNIdApwObGyxXuBfue/KZDN8+H+exOzrHyYagiNOOZwDhven6p5nGXToQA4Zc3BXd9G6QEM0cOeSWVx9/JfJKMP82qeo3fwGnxr5CV59awXPrnmBCQefwgcGj2ZnNLB5xxbueOGXAATBr19+gGuPvxwkXn1rBfNW/qGLj2jvkCnSqaqIqJc0FZhL4yVjMyNisaTpQHVEVAJTJZ0G7KAxJy8s1K4idv/Xv6SfA7+IiF3+b0u6KyLOb+VrzewL0wvWcVV/Xd7VXbASdPfE2/d4mFq97ql2Z86YgSclfnayzZFuRFzcxraCgWtmlrRSv8rDN0eYWar4gTdmZgnySNfMLEEOXTOzBPkh5mZmCfJI18wsQT6RZmaWII90zcwS5JGumVmCPNI1M0uQr14wM0uQR7pmZgly6JqZJcgn0szMEuXQNTNLjE+kmZklyHO6ZmYJKvU53dIeh5uZdZA68K9gW9IESUsl1Uia1sr2r0laIukFSY9JKvhCRIeumaVKsUJXUhaYAUwERgHnSRrVotqfgDERMRq4H7i5UP8cumaWKpLaXQoYC9RExPKIqANmAZPzK0TE4xGxNbe4CCgr1KhD18xSJaNMu4ukcknVeaU8r6lhwMq85drcut25GHi4UP98Is3MUqUjVy9ERAVQscf7lD4HjAFOLlTXoWtmKVO0qxdWAcPzlsty65rvTToNuAY4OSLeKdSopxfMLFXUgVJAFTBS0ghJ3YEpQGWzfUnHAT8BJkXEmvb0zyNdM0uVYl2nGxH1kqYCc4EsMDMiFkuaDlRHRCVwC9AbuC+33xURMamtdh26ZpYyxbs5IiJmA7NbrLsu7/NpHW3ToWtmqeLbgM3MEuTbgM3MrIlHumaWKp5eMDNLkEPXzCxBntM1M7MmHumaWap4esHMLFEOXTOzxJR25Dp0zSxlSv1EmkPXzFLFc7pmZoly6JqZJabUpxd8na6ZWYI80jWzVPGcrplZohy6ZmaJyXhO18wsScV7NaWkCZKWSqqRNK2V7R+R9EdJ9ZI+1Z7eOXTNLFWKFbmSssAMYCIwCjhP0qgW1VYAFwF3tbd/nl4ws5Qp2vTCWKAmIpYDSJoFTAaWvFshIl7LbWtob6Me6ZpZqkjqSCmXVJ1XyvOaGgaszFuuza3bIx7pmlmqdOSSsYioACo6rze76vTQ/doxXy/tU4kJklSe+59sx3R1B0qHfy6Kq0e2Z7EyZxUwPG+5LLduj3h6IVnlhavYPsg/F6WpChgpaYSk7sAUoHJPG3Xompm1IiLqganAXOAl4N6IWCxpuqRJAJKOl1QLnAv8RNLiQu0qIjqz35ZHUnVEjOnqflhp8c/FvsUj3WR53s5a45+LfYhHumZmCfJI18wsQQ5dM7MEOXQTUujBGbbvkTRT0hpJf+7qvlhyHLoJaOeDM2zfcycwoas7Ycly6Caj6cEZEVEHvPvgDNuHRcSTwIau7ocly6GbjE55cIaZ7X0cumZmCXLoJqNTHpxhZnsfh24yOuXBGWa293HoJmB3D87o2l5ZV5N0N7AQOEJSraSLu7pP1vl8G7CZWYI80jUzS5BD18wsQQ5dM7MEOXTNzBLk0DUzS5BD18wsQQ5dM7ME/X83eJrBTob7pwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4PzNv2LVpYt"
      },
      "source": [
        "l1 = 1e-4\n",
        "l2 = 1e-3\n",
        "bias = 1e-4\n",
        "dropout500=0.4\n",
        "dropout300=0.2\n",
        "dropout200=0.2\n",
        "\n",
        "model1 = keras.Sequential()\n",
        "model1.add(Input((X.shape[1])))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# optimizer = optimizers.SGD(learning_rate=.1)\n",
        "optimizer = keras.optimizers.Adam(learning_rate = .001)\n",
        "\n",
        "model1.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "class_balance = sum(y_t == 1) / sum(y_t == 0)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0001)\n",
        "\n",
        "model1.fit(os_X_t, os_y_t,\n",
        "           batch_size = 200,\n",
        "           epochs = 50,\n",
        "           validation_data=(X_val, y_val),\n",
        "           callbacks = [reduce_lr]\n",
        "           )\n",
        "\n",
        "yhat = np.around(model1.predict(X_val)).astype(int)[:,0]\n",
        "\n",
        "confusion = confusion_matrix(y_val, yhat, normalize='true')\n",
        "\n",
        "heatmap(confusion, cmap='Greens', annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D6V-3IqOWqBU",
        "outputId": "73b3fd46-8e9e-4666-c19f-b9c6e4edeeef"
      },
      "source": [
        "conv_X_t = X_t.to_numpy().reshape(X_t.shape[0], X_t.shape[1], 1)\n",
        "conv_X_val = X_val.to_numpy().reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "\n",
        "\n",
        "input_dims = (None , X_t.shape[1])\n",
        "CNN_model = keras.Sequential()\n",
        "CNN_model.add(Conv1D(filters=40, kernel_size=2, padding='causal', activation='relu'))\n",
        "CNN_model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "CNN_model.add(Dense(50, activation='relu'))\n",
        "CNN_model.add(Dense(50, activation='relu'))\n",
        "CNN_model.add(Dense(50, activation='relu'))\n",
        "CNN_model.add(Dense(50, activation='relu'))\n",
        "\n",
        "CNN_model.add(Dense(25, activation='relu'))\n",
        "CNN_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate = .01)\n",
        "\n",
        "\n",
        "CNN_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "CNN_model.fit(conv_X_t, y_t,\n",
        "          epochs=50,\n",
        "          validation_data=(conv_X_val, y_val),\n",
        "          callbacks = reduce_lr)\n",
        "\n",
        "yhat = np.around(CNN_model.predict(conv_X_val)).astype(int)[:,0]\n",
        "\n",
        "confusion = confusion_matrix(y_val, yhat, normalize='true')\n",
        "\n",
        "heatmap(confusion, cmap='Greens', annot=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "608/608 [==============================] - 4s 6ms/step - loss: 0.7120 - accuracy: 0.6392 - val_loss: 0.6383 - val_accuracy: 0.6391\n",
            "Epoch 2/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6360 - accuracy: 0.6393 - val_loss: 0.6376 - val_accuracy: 0.6391\n",
            "Epoch 3/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6322 - accuracy: 0.6452 - val_loss: 0.6375 - val_accuracy: 0.6391\n",
            "Epoch 4/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6363 - accuracy: 0.6375 - val_loss: 0.6376 - val_accuracy: 0.6391\n",
            "Epoch 5/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6332 - accuracy: 0.6427 - val_loss: 0.6388 - val_accuracy: 0.6391\n",
            "Epoch 6/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6339 - accuracy: 0.6403 - val_loss: 0.6390 - val_accuracy: 0.6391\n",
            "Epoch 7/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6351 - accuracy: 0.6376 - val_loss: 0.6413 - val_accuracy: 0.6391\n",
            "Epoch 8/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6340 - accuracy: 0.6413 - val_loss: 0.6372 - val_accuracy: 0.6391\n",
            "Epoch 9/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6317 - accuracy: 0.6435 - val_loss: 0.6378 - val_accuracy: 0.6391\n",
            "Epoch 10/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6349 - accuracy: 0.6392 - val_loss: 0.6374 - val_accuracy: 0.6391\n",
            "Epoch 11/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6360 - accuracy: 0.6364 - val_loss: 0.6382 - val_accuracy: 0.6391\n",
            "Epoch 12/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6386 - accuracy: 0.6336 - val_loss: 0.6372 - val_accuracy: 0.6391\n",
            "Epoch 13/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6363 - accuracy: 0.6370 - val_loss: 0.6376 - val_accuracy: 0.6391\n",
            "Epoch 14/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6330 - accuracy: 0.6412 - val_loss: 0.6372 - val_accuracy: 0.6391\n",
            "Epoch 15/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6358 - accuracy: 0.6331 - val_loss: 0.6367 - val_accuracy: 0.6391\n",
            "Epoch 16/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6358 - accuracy: 0.6368 - val_loss: 0.6377 - val_accuracy: 0.6391\n",
            "Epoch 17/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6315 - accuracy: 0.6429 - val_loss: 0.6370 - val_accuracy: 0.6391\n",
            "Epoch 18/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6332 - accuracy: 0.6390 - val_loss: 0.6370 - val_accuracy: 0.6391\n",
            "Epoch 19/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6308 - accuracy: 0.6438 - val_loss: 0.6374 - val_accuracy: 0.6391\n",
            "Epoch 20/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6368 - accuracy: 0.6346 - val_loss: 0.6371 - val_accuracy: 0.6391\n",
            "Epoch 21/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6387 - accuracy: 0.6290 - val_loss: 0.6369 - val_accuracy: 0.6391\n",
            "Epoch 22/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6343 - accuracy: 0.6353 - val_loss: 0.6368 - val_accuracy: 0.6391\n",
            "Epoch 23/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6368 - accuracy: 0.6336 - val_loss: 0.6368 - val_accuracy: 0.6391\n",
            "Epoch 24/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6322 - accuracy: 0.6391 - val_loss: 0.6367 - val_accuracy: 0.6391\n",
            "Epoch 25/50\n",
            "608/608 [==============================] - 3s 5ms/step - loss: 0.6304 - accuracy: 0.6437 - val_loss: 0.6366 - val_accuracy: 0.6391\n",
            "Epoch 26/50\n",
            "223/608 [==========>...................] - ETA: 1s - loss: 0.6289 - accuracy: 0.6488"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-226dac4c2af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_X_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           callbacks = reduce_lr)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_X_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X4NDVWienU9s",
        "outputId": "6e2b08ca-250a-47bf-a647-ff3c00fd11b7"
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
        "                              patience=5)\n",
        "\n",
        "conv_X_t = X_t.to_numpy().reshape(X_t.shape[0], X_t.shape[1], 1)\n",
        "conv_X_val = X_val.to_numpy().reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "\n",
        "tf.executing_eagerly()\n",
        "\n",
        "input_dims = (None , X_t.shape[1])\n",
        "CNNLSTM_model = keras.Sequential()\n",
        "CNNLSTM_model.add(Conv1D(filters=10, kernel_size=7, padding='valid', activation='relu'))\n",
        "CNNLSTM_model.add(LSTM(25, activation='tanh', return_sequences=True, use_bias=True))\n",
        "CNNLSTM_model.add(Dense(50, activation='relu'))\n",
        "CNNLSTM_model.add(Dense(50, activation='relu'))\n",
        "CNNLSTM_model.add(Dense(50, activation='relu'))\n",
        "CNNLSTM_model.add(Dense(50, activation='relu'))\n",
        "CNNLSTM_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate = .001)\n",
        "\n",
        "\n",
        "CNNLSTM_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "CNNLSTM_model.fit(conv_X_t, y_t,\n",
        "          epochs=50,\n",
        "          batch_size=100,\n",
        "          validation_data=(conv_X_val, y_val),\n",
        "          callbacks = reduce_lr)\n",
        "\n",
        "yhat = np.around(CNNLSTM_model.predict(conv_X_val)).astype(int)[:,0]\n",
        "\n",
        "confusion = confusion_matrix(y_val, yhat, normalize='true')\n",
        "\n",
        "heatmap(confusion, cmap='Greens', annot=True)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "195/195 [==============================] - 8s 30ms/step - loss: 0.6175 - accuracy: 0.6674 - val_loss: 0.5874 - val_accuracy: 0.7071\n",
            "Epoch 2/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5613 - accuracy: 0.7232 - val_loss: 0.5423 - val_accuracy: 0.7380\n",
            "Epoch 3/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5519 - accuracy: 0.7240 - val_loss: 0.6150 - val_accuracy: 0.6612\n",
            "Epoch 4/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5976 - accuracy: 0.6836 - val_loss: 0.5987 - val_accuracy: 0.6935\n",
            "Epoch 5/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5836 - accuracy: 0.7038 - val_loss: 0.5863 - val_accuracy: 0.7012\n",
            "Epoch 6/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5735 - accuracy: 0.7117 - val_loss: 0.5992 - val_accuracy: 0.6860\n",
            "Epoch 7/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5850 - accuracy: 0.6997 - val_loss: 0.5854 - val_accuracy: 0.7072\n",
            "Epoch 8/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5647 - accuracy: 0.7197 - val_loss: 0.5871 - val_accuracy: 0.7022\n",
            "Epoch 9/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5669 - accuracy: 0.7171 - val_loss: 0.5771 - val_accuracy: 0.7099\n",
            "Epoch 10/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5631 - accuracy: 0.7194 - val_loss: 0.5780 - val_accuracy: 0.7110\n",
            "Epoch 11/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5626 - accuracy: 0.7209 - val_loss: 0.5738 - val_accuracy: 0.7132\n",
            "Epoch 12/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5600 - accuracy: 0.7221 - val_loss: 0.5752 - val_accuracy: 0.7111\n",
            "Epoch 13/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5598 - accuracy: 0.7234 - val_loss: 0.5712 - val_accuracy: 0.7154\n",
            "Epoch 14/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5564 - accuracy: 0.7249 - val_loss: 0.5656 - val_accuracy: 0.7189\n",
            "Epoch 15/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5469 - accuracy: 0.7313 - val_loss: 0.5592 - val_accuracy: 0.7196\n",
            "Epoch 16/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5406 - accuracy: 0.7303 - val_loss: 0.5485 - val_accuracy: 0.7359\n",
            "Epoch 17/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5245 - accuracy: 0.7556 - val_loss: 0.5660 - val_accuracy: 0.7152\n",
            "Epoch 18/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5340 - accuracy: 0.7408 - val_loss: 0.5477 - val_accuracy: 0.7310\n",
            "Epoch 19/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5272 - accuracy: 0.7462 - val_loss: 0.5486 - val_accuracy: 0.7363\n",
            "Epoch 20/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5243 - accuracy: 0.7508 - val_loss: 0.5438 - val_accuracy: 0.7362\n",
            "Epoch 21/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5242 - accuracy: 0.7498 - val_loss: 0.5421 - val_accuracy: 0.7402\n",
            "Epoch 22/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5211 - accuracy: 0.7581 - val_loss: 0.5453 - val_accuracy: 0.7484\n",
            "Epoch 23/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5214 - accuracy: 0.7560 - val_loss: 0.5503 - val_accuracy: 0.7289\n",
            "Epoch 24/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5263 - accuracy: 0.7443 - val_loss: 0.5442 - val_accuracy: 0.7351\n",
            "Epoch 25/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5263 - accuracy: 0.7471 - val_loss: 0.5391 - val_accuracy: 0.7421\n",
            "Epoch 26/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5176 - accuracy: 0.7584 - val_loss: 0.5438 - val_accuracy: 0.7379\n",
            "Epoch 27/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5212 - accuracy: 0.7521 - val_loss: 0.5393 - val_accuracy: 0.7428\n",
            "Epoch 28/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5206 - accuracy: 0.7553 - val_loss: 0.5367 - val_accuracy: 0.7451\n",
            "Epoch 29/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5169 - accuracy: 0.7623 - val_loss: 0.5340 - val_accuracy: 0.7490\n",
            "Epoch 30/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5106 - accuracy: 0.7605 - val_loss: 0.5396 - val_accuracy: 0.7422\n",
            "Epoch 31/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5205 - accuracy: 0.7575 - val_loss: 0.5361 - val_accuracy: 0.7458\n",
            "Epoch 32/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5214 - accuracy: 0.7565 - val_loss: 0.5396 - val_accuracy: 0.7505\n",
            "Epoch 33/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5202 - accuracy: 0.7543 - val_loss: 0.5420 - val_accuracy: 0.7390\n",
            "Epoch 34/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5128 - accuracy: 0.7594 - val_loss: 0.5392 - val_accuracy: 0.7406\n",
            "Epoch 35/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5174 - accuracy: 0.7557 - val_loss: 0.5350 - val_accuracy: 0.7444\n",
            "Epoch 36/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5183 - accuracy: 0.7579 - val_loss: 0.5355 - val_accuracy: 0.7524\n",
            "Epoch 37/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5145 - accuracy: 0.7680 - val_loss: 0.5357 - val_accuracy: 0.7524\n",
            "Epoch 38/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5181 - accuracy: 0.7626 - val_loss: 0.5329 - val_accuracy: 0.7510\n",
            "Epoch 39/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5149 - accuracy: 0.7635 - val_loss: 0.5353 - val_accuracy: 0.7548\n",
            "Epoch 40/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5231 - accuracy: 0.7631 - val_loss: 0.5340 - val_accuracy: 0.7553\n",
            "Epoch 41/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5141 - accuracy: 0.7679 - val_loss: 0.5322 - val_accuracy: 0.7561\n",
            "Epoch 42/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5210 - accuracy: 0.7618 - val_loss: 0.5314 - val_accuracy: 0.7529\n",
            "Epoch 43/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5056 - accuracy: 0.7704 - val_loss: 0.5312 - val_accuracy: 0.7497\n",
            "Epoch 44/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5088 - accuracy: 0.7659 - val_loss: 0.5305 - val_accuracy: 0.7506\n",
            "Epoch 45/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5138 - accuracy: 0.7615 - val_loss: 0.5296 - val_accuracy: 0.7517\n",
            "Epoch 46/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5176 - accuracy: 0.7615 - val_loss: 0.5316 - val_accuracy: 0.7519\n",
            "Epoch 47/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5074 - accuracy: 0.7709 - val_loss: 0.5311 - val_accuracy: 0.7511\n",
            "Epoch 48/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5158 - accuracy: 0.7609 - val_loss: 0.5342 - val_accuracy: 0.7519\n",
            "Epoch 49/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5097 - accuracy: 0.7683 - val_loss: 0.5342 - val_accuracy: 0.7522\n",
            "Epoch 50/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5152 - accuracy: 0.7678 - val_loss: 0.5341 - val_accuracy: 0.7554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f92f4045690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4v_AJcSaVua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9be040d0-3fcd-49b1-9139-6f780579b18b"
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
        "                              patience=5)\n",
        "\n",
        "conv_X_t = X_t.to_numpy().reshape(X_t.shape[0], X_t.shape[1], 1)\n",
        "conv_X_val = X_val.to_numpy().reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "\n",
        "tf.executing_eagerly()\n",
        "\n",
        "input_dims = (None , X_t.shape[1])\n",
        "LSTM_model = keras.Sequential()\n",
        "LSTM_model.add(LSTM(25, activation='tanh', return_sequences=True, use_bias=True))\n",
        "LSTM_model.add(Dense(50, activation='relu'))\n",
        "LSTM_model.add(Dense(50, activation='relu'))\n",
        "LSTM_model.add(Dense(50, activation='relu'))\n",
        "LSTM_model.add(Dense(50, activation='relu'))\n",
        "LSTM_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate = .001)\n",
        "\n",
        "\n",
        "LSTM_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "LSTM_model.fit(conv_X_t, y_t,\n",
        "          epochs=50,\n",
        "          batch_size=100,\n",
        "          validation_data=(conv_X_val, y_val),\n",
        "          callbacks = reduce_lr)\n",
        "\n",
        "yhat = np.around(LSTM_model.predict(conv_X_val)).astype(int)[:,0]\n",
        "\n",
        "confusion = confusion_matrix(y_val, yhat, normalize='true')\n",
        "\n",
        "heatmap(confusion, cmap='Greens', annot=True)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "195/195 [==============================] - 8s 30ms/step - loss: 0.6243 - accuracy: 0.6569 - val_loss: 0.5871 - val_accuracy: 0.7043\n",
            "Epoch 2/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5809 - accuracy: 0.7089 - val_loss: 0.6300 - val_accuracy: 0.7315\n",
            "Epoch 3/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5513 - accuracy: 0.7343 - val_loss: 0.5276 - val_accuracy: 0.7601\n",
            "Epoch 4/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5337 - accuracy: 0.7463 - val_loss: 0.5322 - val_accuracy: 0.7448\n",
            "Epoch 5/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5184 - accuracy: 0.7619 - val_loss: 0.5302 - val_accuracy: 0.7531\n",
            "Epoch 6/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5777 - accuracy: 0.7263 - val_loss: 0.5677 - val_accuracy: 0.7072\n",
            "Epoch 7/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5402 - accuracy: 0.7351 - val_loss: 0.5197 - val_accuracy: 0.7700\n",
            "Epoch 8/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.5089 - accuracy: 0.7735 - val_loss: 0.5148 - val_accuracy: 0.7738\n",
            "Epoch 9/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.5285 - accuracy: 0.7495 - val_loss: 0.5204 - val_accuracy: 0.7727\n",
            "Epoch 10/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5467 - accuracy: 0.7355 - val_loss: 0.5693 - val_accuracy: 0.7167\n",
            "Epoch 11/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5594 - accuracy: 0.7208 - val_loss: 0.5453 - val_accuracy: 0.7440\n",
            "Epoch 12/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5402 - accuracy: 0.7304 - val_loss: 0.5391 - val_accuracy: 0.7463\n",
            "Epoch 13/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5134 - accuracy: 0.7620 - val_loss: 0.5462 - val_accuracy: 0.7376\n",
            "Epoch 14/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5243 - accuracy: 0.7499 - val_loss: 0.5302 - val_accuracy: 0.7541\n",
            "Epoch 15/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5016 - accuracy: 0.7743 - val_loss: 0.5157 - val_accuracy: 0.7659\n",
            "Epoch 16/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4998 - accuracy: 0.7722 - val_loss: 0.5077 - val_accuracy: 0.7743\n",
            "Epoch 17/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5075 - accuracy: 0.7688 - val_loss: 0.5083 - val_accuracy: 0.7718\n",
            "Epoch 18/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.4994 - accuracy: 0.7749 - val_loss: 0.5120 - val_accuracy: 0.7746\n",
            "Epoch 19/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5195 - accuracy: 0.7576 - val_loss: 0.5633 - val_accuracy: 0.7187\n",
            "Epoch 20/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5472 - accuracy: 0.7295 - val_loss: 0.5661 - val_accuracy: 0.7165\n",
            "Epoch 21/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5480 - accuracy: 0.7299 - val_loss: 0.5524 - val_accuracy: 0.7292\n",
            "Epoch 22/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5317 - accuracy: 0.7437 - val_loss: 0.5494 - val_accuracy: 0.7307\n",
            "Epoch 23/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5390 - accuracy: 0.7376 - val_loss: 0.5275 - val_accuracy: 0.7233\n",
            "Epoch 24/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5121 - accuracy: 0.7364 - val_loss: 0.5236 - val_accuracy: 0.7356\n",
            "Epoch 25/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.5027 - accuracy: 0.7397 - val_loss: 0.5116 - val_accuracy: 0.7611\n",
            "Epoch 26/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4958 - accuracy: 0.7684 - val_loss: 0.5014 - val_accuracy: 0.7747\n",
            "Epoch 27/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4921 - accuracy: 0.7759 - val_loss: 0.5045 - val_accuracy: 0.7764\n",
            "Epoch 28/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.4962 - accuracy: 0.7690 - val_loss: 0.5078 - val_accuracy: 0.7567\n",
            "Epoch 29/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4930 - accuracy: 0.7651 - val_loss: 0.5065 - val_accuracy: 0.7573\n",
            "Epoch 30/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.4931 - accuracy: 0.7650 - val_loss: 0.5075 - val_accuracy: 0.7640\n",
            "Epoch 31/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.4911 - accuracy: 0.7717 - val_loss: 0.5128 - val_accuracy: 0.7715\n",
            "Epoch 32/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.4997 - accuracy: 0.7776 - val_loss: 0.5100 - val_accuracy: 0.7709\n",
            "Epoch 33/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.4953 - accuracy: 0.7764 - val_loss: 0.5482 - val_accuracy: 0.7277\n",
            "Epoch 34/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.5042 - accuracy: 0.7404 - val_loss: 0.5030 - val_accuracy: 0.7488\n",
            "Epoch 35/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.4840 - accuracy: 0.7584 - val_loss: 0.4987 - val_accuracy: 0.7541\n",
            "Epoch 36/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4794 - accuracy: 0.7623 - val_loss: 0.4972 - val_accuracy: 0.7550\n",
            "Epoch 37/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4836 - accuracy: 0.7618 - val_loss: 0.4944 - val_accuracy: 0.7587\n",
            "Epoch 38/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.4835 - accuracy: 0.7643 - val_loss: 0.4975 - val_accuracy: 0.7595\n",
            "Epoch 39/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4796 - accuracy: 0.7705 - val_loss: 0.4948 - val_accuracy: 0.7651\n",
            "Epoch 40/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.4785 - accuracy: 0.7723 - val_loss: 0.4936 - val_accuracy: 0.7662\n",
            "Epoch 41/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4854 - accuracy: 0.7740 - val_loss: 0.5016 - val_accuracy: 0.7712\n",
            "Epoch 42/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4867 - accuracy: 0.7801 - val_loss: 0.5004 - val_accuracy: 0.7731\n",
            "Epoch 43/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4838 - accuracy: 0.7818 - val_loss: 0.4971 - val_accuracy: 0.7703\n",
            "Epoch 44/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4720 - accuracy: 0.7795 - val_loss: 0.4953 - val_accuracy: 0.7673\n",
            "Epoch 45/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.4719 - accuracy: 0.7739 - val_loss: 0.4904 - val_accuracy: 0.7771\n",
            "Epoch 46/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4727 - accuracy: 0.7781 - val_loss: 0.4879 - val_accuracy: 0.7747\n",
            "Epoch 47/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4767 - accuracy: 0.7742 - val_loss: 0.4877 - val_accuracy: 0.7665\n",
            "Epoch 48/50\n",
            "195/195 [==============================] - 5s 28ms/step - loss: 0.4738 - accuracy: 0.7733 - val_loss: 0.4892 - val_accuracy: 0.7705\n",
            "Epoch 49/50\n",
            "195/195 [==============================] - 6s 28ms/step - loss: 0.4688 - accuracy: 0.7793 - val_loss: 0.4880 - val_accuracy: 0.7681\n",
            "Epoch 50/50\n",
            "195/195 [==============================] - 6s 29ms/step - loss: 0.4720 - accuracy: 0.7727 - val_loss: 0.4887 - val_accuracy: 0.7745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f91b11e8ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wPYBskRPZVxl",
        "outputId": "d787638e-dd1e-42cd-b600-5315088c7c91"
      },
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                              patience=5)\n",
        "\n",
        "l1 = 1e-4\n",
        "l2 = 1e-2\n",
        "bias = 1e-4\n",
        "dropout500=0.4\n",
        "dropout300=0.3\n",
        "dropout200=0.3\n",
        "\n",
        "model1 = keras.Sequential()\n",
        "model1.add(Input((X.shape[1])))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(500, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout500))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(300, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout300))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "model1.add(Dropout(rate=dropout200))\n",
        "model1.add(Dense(200, activation='relu', \n",
        "                 kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2),\n",
        "                 bias_regularizer=regularizers.l2(bias)))\n",
        "\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate = .001)\n",
        "\n",
        "\n",
        "model1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model1.fit(os_X_t, os_y_t,\n",
        "          epochs=100,\n",
        "          batch_size=10,\n",
        "          validation_data=(X_val, y_val),\n",
        "          callbacks = reduce_lr)\n",
        "\n",
        "yhat = np.around(model1.predict(X_val)).astype(int)[:,0]\n",
        "\n",
        "confusion = confusion_matrix(y_val, yhat, normalize='true')\n",
        "heatmap(confusion, cmap='Greens', annot=True)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2489/2489 [==============================] - 20s 7ms/step - loss: 23.4419 - accuracy: 0.6202 - val_loss: 0.9539 - val_accuracy: 0.7325\n",
            "Epoch 2/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.8555 - accuracy: 0.7168 - val_loss: 0.7627 - val_accuracy: 0.7395\n",
            "Epoch 3/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.7907 - accuracy: 0.7196 - val_loss: 0.7723 - val_accuracy: 0.7566\n",
            "Epoch 4/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.8032 - accuracy: 0.7193 - val_loss: 0.8746 - val_accuracy: 0.6557\n",
            "Epoch 5/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.8091 - accuracy: 0.7185 - val_loss: 0.7510 - val_accuracy: 0.7612\n",
            "Epoch 6/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.8040 - accuracy: 0.7231 - val_loss: 0.9169 - val_accuracy: 0.6104\n",
            "Epoch 7/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.8019 - accuracy: 0.7174 - val_loss: 0.7441 - val_accuracy: 0.7714\n",
            "Epoch 8/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.7964 - accuracy: 0.7263 - val_loss: 0.7600 - val_accuracy: 0.7571\n",
            "Epoch 9/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.7209 - accuracy: 0.7323 - val_loss: 0.6697 - val_accuracy: 0.7723\n",
            "Epoch 10/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6847 - accuracy: 0.7427 - val_loss: 0.6561 - val_accuracy: 0.7839\n",
            "Epoch 11/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6709 - accuracy: 0.7520 - val_loss: 0.6533 - val_accuracy: 0.7825\n",
            "Epoch 12/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6743 - accuracy: 0.7516 - val_loss: 0.6489 - val_accuracy: 0.7876\n",
            "Epoch 13/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6697 - accuracy: 0.7539 - val_loss: 0.6461 - val_accuracy: 0.7867\n",
            "Epoch 14/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6696 - accuracy: 0.7515 - val_loss: 0.6502 - val_accuracy: 0.7857\n",
            "Epoch 15/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6607 - accuracy: 0.7591 - val_loss: 0.6405 - val_accuracy: 0.7871\n",
            "Epoch 16/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6601 - accuracy: 0.7561 - val_loss: 0.6374 - val_accuracy: 0.7862\n",
            "Epoch 17/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6587 - accuracy: 0.7545 - val_loss: 0.6404 - val_accuracy: 0.7881\n",
            "Epoch 18/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6637 - accuracy: 0.7505 - val_loss: 0.6412 - val_accuracy: 0.7899\n",
            "Epoch 19/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6605 - accuracy: 0.7542 - val_loss: 0.6425 - val_accuracy: 0.7904\n",
            "Epoch 20/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6560 - accuracy: 0.7599 - val_loss: 0.6501 - val_accuracy: 0.7890\n",
            "Epoch 21/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6605 - accuracy: 0.7572 - val_loss: 0.6412 - val_accuracy: 0.7867\n",
            "Epoch 22/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6579 - accuracy: 0.7590 - val_loss: 0.6453 - val_accuracy: 0.7881\n",
            "Epoch 23/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6564 - accuracy: 0.7595 - val_loss: 0.6384 - val_accuracy: 0.7876\n",
            "Epoch 24/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6472 - accuracy: 0.7594 - val_loss: 0.6343 - val_accuracy: 0.7881\n",
            "Epoch 25/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6483 - accuracy: 0.7642 - val_loss: 0.6346 - val_accuracy: 0.7890\n",
            "Epoch 26/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6444 - accuracy: 0.7601 - val_loss: 0.6331 - val_accuracy: 0.7899\n",
            "Epoch 27/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6452 - accuracy: 0.7615 - val_loss: 0.6332 - val_accuracy: 0.7899\n",
            "Epoch 28/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6438 - accuracy: 0.7595 - val_loss: 0.6334 - val_accuracy: 0.7899\n",
            "Epoch 29/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6422 - accuracy: 0.7635 - val_loss: 0.6335 - val_accuracy: 0.7913\n",
            "Epoch 30/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6405 - accuracy: 0.7629 - val_loss: 0.6335 - val_accuracy: 0.7913\n",
            "Epoch 31/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6416 - accuracy: 0.7628 - val_loss: 0.6334 - val_accuracy: 0.7922\n",
            "Epoch 32/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6386 - accuracy: 0.7660 - val_loss: 0.6328 - val_accuracy: 0.7899\n",
            "Epoch 33/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6390 - accuracy: 0.7635 - val_loss: 0.6307 - val_accuracy: 0.7936\n",
            "Epoch 34/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6440 - accuracy: 0.7613 - val_loss: 0.6322 - val_accuracy: 0.7908\n",
            "Epoch 35/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6436 - accuracy: 0.7612 - val_loss: 0.6320 - val_accuracy: 0.7927\n",
            "Epoch 36/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6376 - accuracy: 0.7673 - val_loss: 0.6316 - val_accuracy: 0.7913\n",
            "Epoch 37/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6419 - accuracy: 0.7620 - val_loss: 0.6300 - val_accuracy: 0.7918\n",
            "Epoch 38/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6309 - accuracy: 0.7727 - val_loss: 0.6314 - val_accuracy: 0.7927\n",
            "Epoch 39/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6402 - accuracy: 0.7655 - val_loss: 0.6319 - val_accuracy: 0.7955\n",
            "Epoch 40/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6353 - accuracy: 0.7691 - val_loss: 0.6317 - val_accuracy: 0.7941\n",
            "Epoch 41/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6417 - accuracy: 0.7616 - val_loss: 0.6321 - val_accuracy: 0.7932\n",
            "Epoch 42/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6321 - accuracy: 0.7710 - val_loss: 0.6316 - val_accuracy: 0.7936\n",
            "Epoch 43/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6315 - accuracy: 0.7684 - val_loss: 0.6313 - val_accuracy: 0.7936\n",
            "Epoch 44/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6367 - accuracy: 0.7661 - val_loss: 0.6311 - val_accuracy: 0.7936\n",
            "Epoch 45/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6252 - accuracy: 0.7784 - val_loss: 0.6311 - val_accuracy: 0.7936\n",
            "Epoch 46/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6358 - accuracy: 0.7693 - val_loss: 0.6310 - val_accuracy: 0.7922\n",
            "Epoch 47/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6402 - accuracy: 0.7665 - val_loss: 0.6309 - val_accuracy: 0.7932\n",
            "Epoch 48/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6340 - accuracy: 0.7676 - val_loss: 0.6307 - val_accuracy: 0.7927\n",
            "Epoch 49/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6354 - accuracy: 0.7662 - val_loss: 0.6308 - val_accuracy: 0.7932\n",
            "Epoch 50/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6331 - accuracy: 0.7705 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 51/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6299 - accuracy: 0.7747 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 52/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6339 - accuracy: 0.7678 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 53/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6327 - accuracy: 0.7717 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 54/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6365 - accuracy: 0.7653 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 55/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6349 - accuracy: 0.7660 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 56/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6296 - accuracy: 0.7704 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 57/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6463 - accuracy: 0.7587 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 58/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6309 - accuracy: 0.7709 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 59/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6383 - accuracy: 0.7642 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 60/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6317 - accuracy: 0.7703 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 61/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6385 - accuracy: 0.7648 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 62/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6400 - accuracy: 0.7631 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 63/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6341 - accuracy: 0.7648 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 64/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6338 - accuracy: 0.7681 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 65/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6360 - accuracy: 0.7685 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 66/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6348 - accuracy: 0.7650 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 67/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6322 - accuracy: 0.7705 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 68/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6330 - accuracy: 0.7703 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 69/100\n",
            "2489/2489 [==============================] - 17s 7ms/step - loss: 0.6337 - accuracy: 0.7700 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 70/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6335 - accuracy: 0.7704 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 71/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6365 - accuracy: 0.7657 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 72/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6380 - accuracy: 0.7622 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 73/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6285 - accuracy: 0.7726 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 74/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6340 - accuracy: 0.7699 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 75/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6346 - accuracy: 0.7701 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 76/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6343 - accuracy: 0.7679 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 77/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6337 - accuracy: 0.7693 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 78/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6364 - accuracy: 0.7657 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 79/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6346 - accuracy: 0.7651 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 80/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6361 - accuracy: 0.7687 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 81/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6345 - accuracy: 0.7704 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 82/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6329 - accuracy: 0.7685 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 83/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6312 - accuracy: 0.7690 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 84/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6374 - accuracy: 0.7684 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 85/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6384 - accuracy: 0.7682 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 86/100\n",
            "2489/2489 [==============================] - 16s 7ms/step - loss: 0.6333 - accuracy: 0.7691 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 87/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6304 - accuracy: 0.7697 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 88/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6353 - accuracy: 0.7698 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 89/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6337 - accuracy: 0.7702 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 90/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6359 - accuracy: 0.7692 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 91/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6346 - accuracy: 0.7666 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 92/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6320 - accuracy: 0.7677 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 93/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6402 - accuracy: 0.7638 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 94/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6325 - accuracy: 0.7720 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 95/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6313 - accuracy: 0.7721 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 96/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6378 - accuracy: 0.7644 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 97/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6373 - accuracy: 0.7642 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 98/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6344 - accuracy: 0.7675 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 99/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6347 - accuracy: 0.7686 - val_loss: 0.6307 - val_accuracy: 0.7932\n",
            "Epoch 100/100\n",
            "2489/2489 [==============================] - 16s 6ms/step - loss: 0.6339 - accuracy: 0.7713 - val_loss: 0.6307 - val_accuracy: 0.7932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9326bad490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p62Haxo0cmLO",
        "outputId": "a5c60d0e-c22d-45f6-dfe1-5d3a4baf8abb"
      },
      "source": [
        "y_t"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "registration\n",
              "DDD2013J59370      0\n",
              "CCC2014J655973     0\n",
              "BBB2013J563794     0\n",
              "DDD2013B2536214    1\n",
              "BBB2014B607953     1\n",
              "                  ..\n",
              "FFF2013B494165     0\n",
              "FFF2013B536455     0\n",
              "BBB2014B625249     0\n",
              "BBB2014J692175     1\n",
              "BBB2013J486457     0\n",
              "Name: final_result, Length: 19443, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f0Y0AgbuK0c",
        "outputId": "cac34e37-265f-4ebc-fa6b-1f06eb77735b"
      },
      "source": [
        "df2 = get_timeseries_table(prediction_window=135)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assessments merged:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmHFI0KTt4KH",
        "outputId": "c05950e4-06fe-4ebf-ef6c-4ab5f0b317b2"
      },
      "source": [
        "yhat = np.around(model1.predict(X_val)).astype(int)[:,0]\n",
        "error = pd.DataFrame(y_val.copy())\n",
        "error['pred_result'] = yhat\n",
        "error = error.merge(df2[['final_result']], on='registration', how='left')\n",
        "error[error['final_result_x'] != error['pred_result']]['final_result_y'].value_counts()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Fail           259\n",
              "Withdrawn       93\n",
              "Pass            88\n",
              "Distinction      7\n",
              "Name: final_result_y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17xYUbqFuVhc",
        "outputId": "3c1df32e-bf95-4ce1-eb0d-91a10e708789"
      },
      "source": [
        "error['final_result_y'].value_counts()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pass           1138\n",
              "Fail            580\n",
              "Distinction     243\n",
              "Withdrawn       200\n",
              "Name: final_result_y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "YjGKHAcdzvn_",
        "outputId": "d131d1aa-2db7-4d13-bfbc-05845a6759b0"
      },
      "source": [
        "analysis = pd.DataFrame()\n",
        "analysis['error'] = error[error['final_result_x'] != error['pred_result']]['final_result_y'].value_counts()\n",
        "analysis['total'] = error['final_result_y'].value_counts()\n",
        "analysis"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>error</th>\n",
              "      <th>total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Fail</th>\n",
              "      <td>259</td>\n",
              "      <td>580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Withdrawn</th>\n",
              "      <td>93</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pass</th>\n",
              "      <td>88</td>\n",
              "      <td>1138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distinction</th>\n",
              "      <td>7</td>\n",
              "      <td>243</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             error  total\n",
              "Fail           259    580\n",
              "Withdrawn       93    200\n",
              "Pass            88   1138\n",
              "Distinction      7    243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "plDh8h0Q0BBE",
        "outputId": "9491d55d-53cf-40e0-be5d-f694dfa68766"
      },
      "source": [
        "analysis['error'] = analysis['error']/analysis['error'].sum()\n",
        "analysis['total'] = analysis['total']/analysis['total'].sum()\n",
        "analysis"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>error</th>\n",
              "      <th>total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Fail</th>\n",
              "      <td>0.579418</td>\n",
              "      <td>0.268394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Withdrawn</th>\n",
              "      <td>0.208054</td>\n",
              "      <td>0.092550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pass</th>\n",
              "      <td>0.196868</td>\n",
              "      <td>0.526608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distinction</th>\n",
              "      <td>0.015660</td>\n",
              "      <td>0.112448</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                error     total\n",
              "Fail         0.579418  0.268394\n",
              "Withdrawn    0.208054  0.092550\n",
              "Pass         0.196868  0.526608\n",
              "Distinction  0.015660  0.112448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jKeWAzu0N4d"
      },
      "source": [
        "analysis['ratio'] = analysis['error']/analysis['total']"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "UrcCeUdH0Vwd",
        "outputId": "53949e7f-4523-4a54-d7f6-7e33d6769fad"
      },
      "source": [
        "analysis"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>error</th>\n",
              "      <th>total</th>\n",
              "      <th>ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Fail</th>\n",
              "      <td>0.579418</td>\n",
              "      <td>0.268394</td>\n",
              "      <td>2.158833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Withdrawn</th>\n",
              "      <td>0.208054</td>\n",
              "      <td>0.092550</td>\n",
              "      <td>2.248020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pass</th>\n",
              "      <td>0.196868</td>\n",
              "      <td>0.526608</td>\n",
              "      <td>0.373842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Distinction</th>\n",
              "      <td>0.015660</td>\n",
              "      <td>0.112448</td>\n",
              "      <td>0.139264</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                error     total     ratio\n",
              "Fail         0.579418  0.268394  2.158833\n",
              "Withdrawn    0.208054  0.092550  2.248020\n",
              "Pass         0.196868  0.526608  0.373842\n",
              "Distinction  0.015660  0.112448  0.139264"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJAuU0_m0WZF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}